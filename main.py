import os
import re
import time

from pdf_processor import prepare_cv_text
from chatgpt_client import (
    extract_structure_with_gpt,
    extract_details_with_gpt,
    auto_fix_missing_fields,
)
from skill_mapper import remap_hard_skills
from postprocess import unify_languages, unify_durations, clean_duplicates_in_skills
from utils import save_json, has_empty_fields
from schema import validate_schema


# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
INPUT_PDF = "data_input/CV_Kunde_1.pdf"
OUTPUT_JSON = "data_output/result_1.json"


# ============================================================
# üîπ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ============================================================
def filter_explicit_domains(text: str, domains: list[str]) -> list[str]:
    """
    –†–∞—Å—à–∏—Ä—è–µ—Ç –ø–æ–∏—Å–∫ –¥–æ–º–µ–Ω–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.
    """
    domain_keywords = {
        "Machine Learning": ["machine learning", "ml", "deep learning", "neural network"],
        "AI": ["artificial intelligence", "ai model", "generative ai"],
        "Data Engineering": ["data pipeline", "data ingestion", "etl", "databricks", "snowflake"],
        "MLOps": ["mlops", "model deployment", "ci/cd for models", "vertex ai", "sagemaker"],
        "Cloud": ["aws", "azure", "gcp", "kubernetes", "terraform"],
        "Analytics": ["bi", "power bi", "analytics", "dashboards", "reporting"],
        "IoT": ["iot", "connected devices", "sensor data", "predictive maintenance"],
        "Finance": ["banking", "fintech", "risk model", "insurance"],
        "Healthcare": ["medical", "health", "pharma", "clinical"],
        "Manufacturing": ["factory", "industrial", "process optimization", "production"],
    }

    found = set()
    text_l = text.lower()
    for domain, keywords in domain_keywords.items():
        if any(k in text_l for k in keywords):
            found.add(domain)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –¥–æ–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ GPT —Ç–æ–∂–µ –≤–µ—Ä–Ω—É–ª (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if domains:
        found.update(domains)

    return sorted(found)


def shorten_profile_summary(text: str, max_chars: int = 1200) -> str:
    """
    –û–±—Ä–µ–∑–∞–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è, –µ—Å–ª–∏ GPT –≤—ã–¥–∞–ª —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text.strip())
    if len(text) > max_chars:
        cut = text[:max_chars]
        if "." in cut:
            cut = cut[:cut.rfind(".") + 1]
        return cut.strip()
    return text.strip()


# ============================================================
# üîπ –ì–ª–∞–≤–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
# ============================================================

def main():
    start_time = time.time()
    print("üöÄ Starting CV Extraction & Structuring Pipeline v2.0")

    # 1Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
    prepared_text = prepare_cv_text(INPUT_PDF)

    # 2Ô∏è‚É£ GPT –®–∞–≥ 1 ‚Äî –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    print("\nüß© Step 1: Extracting CV structure...")
    structure = extract_structure_with_gpt(prepared_text)

    # 3Ô∏è‚É£ GPT –®–∞–≥ 2 ‚Äî –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("\nüîç Step 2: Extracting detailed content...")
    result = extract_details_with_gpt(prepared_text, structure)

    # 4Ô∏è‚É£ –ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    print("\nü§ñ Step 3: Auto-filling missing fields...")
    if has_empty_fields(result):
        result = auto_fix_missing_fields(result)

    # 5Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è JSON
    print("\nüìè Step 4: Schema validation...")
    result = validate_schema(result)

    # 6Ô∏è‚É£ –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüßº Step 5: Normalizing and cleaning data...")
    result["hard_skills"] = remap_hard_skills(result.get("hard_skills", {}))
    result["hard_skills"] = clean_duplicates_in_skills(result["hard_skills"])
    result["languages"] = unify_languages(result.get("languages", []), original_text=prepared_text)

    # 7Ô∏è‚É£ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è
    explicit_domains = filter_explicit_domains(prepared_text, result.get("domains", []))
    if explicit_domains:
        result["domains"] = explicit_domains
    result["profile_summary"] = shorten_profile_summary(result.get("profile_summary", ""))

    # 8Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    save_json(OUTPUT_JSON, result)

    elapsed = time.time() - start_time
    print(f"\n‚úÖ Process completed successfully in {elapsed:.2f}s")
    print(f"üíæ Result saved to: {OUTPUT_JSON}")


# ============================================================
# üîπ –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
# ============================================================

if __name__ == "__main__":
    main()
