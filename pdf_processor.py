import fitz  # PyMuPDF
import re
import os
from langdetect import detect, DetectorFactory
from chatgpt_client import ask_chatgpt

DetectorFactory.seed = 0  # –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞


# ============================================================
# 1Ô∏è‚É£ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
# ============================================================
def extract_text_by_page(pdf_path: str) -> list[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ PDF –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ."""
    doc = fitz.open(pdf_path)
    pages_text = []
    for page in doc:
        text = page.get_text("blocks") or page.get_text("text")
        if isinstance(text, list):
            text = "\n".join([b[4] for b in text if b[4].strip()])
        text = re.sub(r"[ \t]+", " ", text)
        text = re.sub(r"\n{2,}", "\n", text)
        pages_text.append(text.strip())
    doc.close()
    return pages_text


# ============================================================
# 2Ô∏è‚É£ –¢–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç
# ============================================================
def tag_dates(text: str) -> str:
    """–ù–∞—Ö–æ–¥–∏—Ç –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–∞—Ç –∏ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –∏—Ö –≤ [DATE]...[/DATE]."""
    date_patterns = [
        r"\b\d{1,2}\.\d{2}\b\s*[-‚Äì]\s*\d{1,2}\.\d{2}\b",          # 07.21 ‚Äì 12.23
        r"\b\d{1,2}\.\d{2}\b\s*[-‚Äì]\s*(?:Jetzt|Aktuell|Heute|Present|Now)\b",
        r"\b\d{2}/\d{4}\s*[-‚Äì]\s*\d{2}/\d{4}\b",                 # 09/2022 ‚Äì 04/2024
        r"\b(20\d{2}|19\d{2})\s*[-‚Äì]\s*(?:20\d{2}|Present|Now|Heute|Jetzt|Aktuell)\b",
        r"\b\d{2}\.\d{2}\s*[-‚Äì]\s*(?:\d{2}\.\d{2}|Jetzt|Aktuell|Heute|Present|Now)\b",
        r"(?:(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*)\s+\d{4}\s*[-‚Äì]\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)?[a-z]*\s*\d{4}",
    ]
    for pattern in date_patterns:
        text = re.sub(pattern, lambda m: f"[DATE]{m.group(0)}[/DATE]", text, flags=re.IGNORECASE)
    return text


# ============================================================
# 3Ô∏è‚É£ –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
# ============================================================
def clean_text(text: str) -> str:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Å–µ–∫—Ü–∏–π CV –¥–ª—è GPT, —Å —è–≤–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏.
    –î–µ–ª–∏—Ç —Ä–µ–∑—é–º–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ä–∞–∑–¥–µ–ª–∞–º: Education, Projects, Skills –∏ —Ç.–¥.
    """
    # –û—á–∏—Å—Ç–∫–∞ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    text = re.sub(r"\[\d+\]|\(\d+\)", "", text)
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\s{2,}", " ", text)

    # üîπ –ö–ª—é—á–µ–≤—ã–µ —Å–µ–∫—Ü–∏–∏
    section_markers = {
        r"(?i)(Domains?|Industries):?": "[DOMAINS]",
        r"(?i)(Languages?|Sprachen|Sprachkenntnisse):?": "[LANGUAGES]",
        r"(?i)(Education|Studium|Ausbildung|Academic Background):?": "[EDUCATION]",
        r"(?i)(Profile|Summary|√úber mich|Professional Summary|Career Summary):": "[PROFILE_SUMMARY]",
        r"(?i)(Projects?|Experience|Berufserfahrung|Employment|Work Experience):?": "[PROJECTS]",
        r"(?i)(Skills|Technologies|Kompetenzen|Tools|Professional skills|Technical skills):?": "[SKILLS]",
    }

    # –í—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –Ω–∞—á–∞–ª–∞ —Å–µ–∫—Ü–∏–∏
    for pattern, marker in section_markers.items():
        text = re.sub(pattern, f"\n{marker}\n\\1", text)

    # üß± –Ø–≤–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å–µ–∫—Ü–∏—é
    tags = ["DOMAINS", "SKILLS", "LANGUAGES", "EDUCATION", "PROJECTS", "PROFILE_SUMMARY"]
    for i, tag in enumerate(tags):
        # –ó–∞–∫—Ä—ã—Ç–∏–µ –¥–æ –Ω–∞—á–∞–ª–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å–µ–∫—Ü–∏–∏
        following_tags = tags[i + 1 :]
        if following_tags:
            next_tag_pattern = "|".join(f"\\[{t}\\]" for t in following_tags)
            text = re.sub(
                rf"\[{tag}\](.*?)(?=\n(?:{next_tag_pattern})|\Z)",
                rf"[{tag}]\1[/{tag}]\n",
                text,
                flags=re.DOTALL,
            )
        else:
            text = re.sub(
                rf"\[{tag}\](.*)",
                rf"[{tag}]\1[/{tag}]\n",
                text,
                flags=re.DOTALL,
            )

    # –£–±–∏—Ä–∞–µ–º "–ø—Ä–∏–∫–ª–µ–µ–Ω–Ω—ã–µ" —Å—Ç—Ä–æ—á–∫–∏ –º–µ–∂–¥—É —Å–µ–∫—Ü–∏—è–º–∏
    text = re.sub(r"\]\s*\[", "]\n\n[", text)

    # üîπ –ü–æ–¥—Å–≤–µ—Ç–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT
    text = re.sub(
        r"\[EDUCATION\]",
        "[EDUCATION]\nContext: These are academic degrees, research or study projects, not employment.\n",
        text,
    )
    text = re.sub(
        r"\[PROJECTS\]",
        "[PROJECTS]\nContext: These are professional or applied projects, often linked to employment or practical experience.\n",
        text,
    )

    return text.strip()


# ============================================================
# 4Ô∏è‚É£ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (semantic tags)
# ============================================================
def normalize_structure(text: str) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –¥–ª—è —Å–ª–∞–±–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ."""
    text = re.sub(r"(?i)\b(profile|about me|summary)\b", "[PROFILE_SUMMARY]", text)
    text = re.sub(r"(?i)\b(experience|employment|projects?|career)\b", "[PROJECTS]", text)
    text = re.sub(r"(?i)\b(education|studies|academic background)\b", "[EDUCATION]", text)
    text = re.sub(r"(?i)\b(skills|technologies|competencies|tools)\b", "[SKILLS]", text)
    text = re.sub(r"(?i)\b(languages|sprachkenntnisse)\b", "[LANGUAGES]", text)
    return text


# ============================================================
# 5Ô∏è‚É£ –û—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π
# ============================================================
def clean_responsibilities(responsibilities, max_words=18, max_items=6):
    """–°–æ–∫—Ä–∞—â–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π."""
    cleaned = []
    for i, resp in enumerate(responsibilities):
        if i >= max_items:
            break
        resp = re.sub(
            r"(?i)\b(responsible\s+for|involved\s+in|participated\s+in|helped\s+to|tasked\s+with|working\s+on|assist(ed)?\s+in|support(ed)?\s+with)\b",
            "",
            resp.strip(),
        ).strip()
        words = resp.split()
        if len(words) > max_words:
            resp = " ".join(words[:max_words]) + "..."
        if resp:
            cleaned.append(resp[0].upper() + resp[1:])
    return cleaned


# ============================================================
# 6Ô∏è‚É£ –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ CV-—Ç–µ–∫—Å—Ç–∞
# ============================================================
def prepare_cv_text(pdf_path: str, cache_dir="data_output") -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π,
    —Ä–∞–∑–º–µ—á–∞–µ—Ç –¥–∞—Ç—ã, –æ—á–∏—â–∞–µ—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–≥–∏ —Å–µ–∫—Ü–∏–π.
    –£–ª—É—á—à–µ–Ω–æ:
    - –Ω–µ —Ç–µ—Ä—è–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–∞—Ç;
    - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å—ã –≤ —Å–µ–∫—Ü–∏—è—Ö;
    - –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–∫–∫—É—Ä–∞—Ç–Ω–æ.
    """
    os.makedirs(cache_dir, exist_ok=True)

    pages = extract_text_by_page(pdf_path)
    raw_text = "\n\n".join(pages)

    # --- –Ø–∑—ã–∫–æ–≤–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è
    try:
        detected_lang = detect(raw_text)
    except Exception:
        detected_lang = "en"

    # --- –ü–µ—Ä–µ–≤–æ–¥ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    if detected_lang != "en":
        translation_prompt = f"""
Translate this CV text to English without summarizing.
Preserve ALL structure, especially date ranges (e.g., "07/2021 ‚Äì 03/2023").
Keep section titles (Education, Languages, Experience) as-is.
TEXT:
{raw_text[:15000]}
"""
        result = ask_chatgpt(translation_prompt, mode="details")
        if isinstance(result, dict) and "raw_response" in result:
            raw_text = result["raw_response"]
        elif isinstance(result, str):
            raw_text = result

    # --- –¢–µ–≥–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–∞—Ç –î–û –æ—á–∏—Å—Ç–∫–∏
    tagged_text = tag_dates(raw_text)

    # --- –ë–µ—Ä–µ–∂–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: —Ä–∞–∑—Ä–µ—à–∞–µ–º —Å–∏–º–≤–æ–ª—ã –¥–ª—è –¥–∞—Ç –∏ –¥–µ—Ñ–∏—Å—ã
    tagged_text = re.sub(r"[^\w\s\.\-/‚Äì‚Äî:,]", " ", tagged_text)
    tagged_text = re.sub(r"\s{3,}", "\n", tagged_text)  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã
    tagged_text = re.sub(r"[ \t]+", " ", tagged_text)
    tagged_text = re.sub(r"\n{2,}", "\n", tagged_text)

    # --- –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ–≥–∏
    cleaned_text = clean_text(tagged_text)
    normalized_text = normalize_structure(cleaned_text)

    # --- –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è GPT (—É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞—Ç)
    normalized_text = (
        "[CV_START]\n"
        "The following is a professional CV. Detect all project durations accurately.\n"
        + normalized_text +
        "\n[CV_END]"
    )

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    with open(os.path.join(cache_dir, "prepared_text.txt"), "w", encoding="utf-8") as f:
        f.write(normalized_text)

    return normalized_text, raw_text

